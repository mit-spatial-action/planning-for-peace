---
title: "Planning for Peace Frequency Study"
author: 
  - name: Eric Robsky Huntley, PhD, GISP
    id: erh
    orcid: "0000-0001-8497-0589"
    email: ehuntley@mit.edu
    affiliation:
      - name: Massachusetts Institute of Technology
        city: Cambridge
        state: MA
        url: www.mit.edu
bibliography: references.bibp
engine: jupyter
---

```{python}
#| echo: false
from repoart.report import OpenAlexReport
from repoart.config import ReportConfig

oar = OpenAlexReport(ReportConfig.from_toml("pfp.toml"))
```

# Data

## Acquisition

I downloaded data from OurResearch's OpenAlex database, an open source knowledge base that is meant to serve as a replacement for e.g., Elsevier's Scopus and Clarivate's Web of Science [@priem_openalex_2022]. I selected OpenAlex for both substantive and technical reasons. On the former, OpenAlex prioritizes indexing across a breadth of disciplines, including those that tend to be less well-served by traditional scientific knowledge bases (humanities, social sciences). It is also larger than either Scopus or Web of Science, indexing 263 million works at time of writing, compared to 97.3 million for Scopus and 92 million for Web of Science. It is also permissively licensed, build on open data, and free to use. Finally, OpenAlex's API is well-documented and served by a maturing Python library (`pyalex`) that is under active development [@de_bruin_pyalex_2025].

In querying the OpenAlex database, my parameters were as follows:

1.  **Query** (`Works().search(...)`): `reparation`, `reparative`, or `repartorial`. Note that OpenAlex does not support wildcard queries (e.g., `repara*`), so all searches must be explicit matches. The `search` method queries across titles, abstracts, and document fulltexts.
2.  **Start Year** (`Works().filter(publication_year="...-..."})`): `1900`, because with experimentation, I observed that the first spike in scholarly activity around reparations followed the First World War.
3.  **End Year** (`Works().filter(publication_year="...-..."})`): `2024`, to avoid including the only partially complete 2025 year.
4.  **Work Type** (`Works().filter(type="...|..."})`): Articles (`article`), books (`book`), or book chapters (`book-chapter`).
5.  **Domains** (`Works().filter(primary_topic={"domain": {"id": "...|..."}})`): Social sciences (which has the code `2`). According to OpenAlex's documentation, humanities scholarship appears here (and inspection of returned works confirms this).
6.  **Languages** (`Works().filter(language="...|..."})`): English (`en`). Currently the study is limited to Anglophone scholarship. This is justified because we are using search-terms and are not currently translating those search terms into other languages, which will lead to a certain amount of bias in search results by giving preference to those with translated keywords.

## Processing

I first normalized text found in work abstracts and titles by lowercasing, removing extraneous spacing, and converting all characters to their unicode (unaccented) equivalents. To identify named entities in work titles and abstracts, I used the `spacy` natural language processing library for Python [@honnibal_spacy_2020]. Specifically, I used its `en_core_web_lg` pipeline, skipping all steps but named entity recognition (`NER`), which is independent in non-transformer (i.e., `_trf`) models.

# Overall Trends

I begin by looking for historical trends in instances of our searchterms from 1900 to the present. I do so by querying OpenAlex using the identified searchterms and grouping by year, and subsequently dividing the resulting instance counts by the total works published matching all query elements except the query string.

As seen below, there are spikes in the proportional frequency of works matching search terms following the first and second world wars, smaller spikes during the height of the decolonial movements of the 1960s and 70s, and a general increase in prevalence since the mid-1990s.

```{python}
#| echo: false
#| warning: false
oar._get_works_prop()
oar._plt_works_prop()
oar.works_prop_plot
```

```{python}
#| echo: false
#| warning: false
oar._get_works()
oar._normalize_works()
oar._recognize_entities()
```


## Histogram

```{python}

from plotnine import *

df = (
    oar.works.explode("title_ents")
    .groupby("title_ents")     # group by year and term
    .size()
    .reset_index(name="count")
    .query("count >= 2") 
    .sort_values("count", ascending=False)
    .rename(columns={"title_ents": "place"})
)

total = df["count"].sum()
df["cum_share"] = df["count"].cumsum() / total

df_top = df[df["cum_share"] <= 0.5]

(
    ggplot(df, aes(x="count"))
    + geom_histogram()
)
```

# Decadal Trends

```{python}
miss = oar._missing_by()

plot = (
    ggplot(miss, aes('year', 'pct_missing')) +
    geom_line()
)

plot
```

```{python}

t = (
    oar.works.explode("title_ents")
    .groupby(["year", "title_ents"])     # group by year and term
    .size()                         # count occurrences
    # .unstack(fill_value=0)
    .reset_index()
)


t['period'] = (t['year'] // 10 ) * 10


```

```{python}
t2 = t.groupby(['period', 'title_ents']).size().unstack(fill_value=0)
```

## Geocoding

In order to normalize recognized entities and categorize them by their types, I first attempted an approach whereby all names were passed to the OpenCage geocoder. This proved (mostly) effective for nations, continents, etc, but broke down for regions, subnational units, placenames (two particularly egregious examples: "Palestine" was assigned a location in Palestine, Texas, USA; "Latin America" was the Rindge & Latin Scool in Cambridge). As such, I took a staged approach.



```{python}
from repoart.loaders import get_ne_states, get_ne_map_units, get_ne_populated_places
from repoart.ner import normalize_columns, strip_spaces, to_lower, remove_extra_spaces, to_unicode

import pandas as pd


states = get_ne_states()

states_norm = normalize_columns(
            states,
            columns=["name", "name_alt"],
            funcs=[strip_spaces, to_lower, remove_extra_spaces, to_unicode]
        )[["adm1_code", "name", "name_alt"]]

map_units = get_ne_map_units()

map_units_norm = normalize_columns(
            map_units,
            columns=["name", "name_long", "brk_name", "postal", "brk_a3", "iso_a2_eh"],
            funcs=[strip_spaces, to_lower, remove_extra_spaces, to_unicode]
        )[["gu_a3", "name", "name_long", "brk_name", "postal", "brk_a3", "iso_a2_eh"]]

map_units_norm["abbrev_list"] = map_units_norm[["postal", "brk_a3", "iso_a2_eh"]].apply(
    lambda x: list(dict.fromkeys(v for v in x if pd.notna(v))),
    axis=1
)

map_units_abbrev = map_units_norm[["gu_a3", "abbrev_list"]].explode("abbrev_list")

map_units_norm['name_list'] = map_units_norm[["name", "name_long", "brk_name"]].apply(
    lambda x: list(dict.fromkeys(v for v in x if pd.notna(v))),
    axis=1
)

map_units_name = map_units_norm[["gu_a3", "name_list"]].explode("name_list")

places = get_ne_populated_places()

places_norm = normalize_columns(
            places,
            columns=["nameascii", "namealt"],
            funcs=[strip_spaces, to_lower, remove_extra_spaces, to_unicode]
        )[["ne_id", "nameascii", "namealt"]]
```

```{python}
t = oar.works[["title_ents"]].explode("title_ents").dropna().reset_index()

t2 = t.merge(map_units_name, left_on="title_ents", right_on="name_list", how="left").set_index("id")
t2 = t.merge(map_units_name, left_on="title_ents", right_on="name_list", how="left").set_index("id")
```


```{python}
import pandas as pd

def progressive_merge(left_df, lookups, target_col, matched_col="matched_from"):
    """
    left_df: DataFrame to enrich
    lookups: list of tuples (left_key, lookup_df, right_key, lookup_value_col)
             lookup_value_col is the column in the lookup table to fill
    target_col: name of the column to fill in left_df
    matched_col: name of the column to track which lookup table matched
    """
    result = left_df.copy()
    idx = result.index.name or "__index__"

    # Initialize columns
    result[target_col] = pd.NA
    result[matched_col] = pd.NA

    for i, (left_key, lookup_df, right_key, lookup_value_col) in enumerate(lookups, start=1):
        mask = result[target_col].isna()
        if not mask.any():
            break

        # Ensure the lookup has the column to assign
        if lookup_value_col not in lookup_df.columns:
            raise ValueError(
                f"Lookup table for {left_key} does not contain column '{lookup_value_col}'"
            )

        merged = (
            result.loc[mask]
            .reset_index(names=idx)
            .merge(
                lookup_df[[right_key, lookup_value_col]],
                left_on=left_key,
                right_on=right_key,
                how="left",
                validate="m:1"
            )
            .set_index(idx)
        )
        print(merged)

        hit = merged[lookup_value_col].notna()

        # Fill the target column and matched_from
        result.loc[mask, target_col] = merged[lookup_value_col]
        result.loc[mask[mask].index[hit], matched_col] = f"lookup_{i}"

    return result
```

```{python}
import pandas as pd
from repoart.ner import normalize_columns, strip_spaces, to_lower, remove_extra_spaces, to_unicode

dfs = [df.merge(countries, left_on="place", right_on=col, how="inner") for col in ["name", "brk_name", "brk_a3"]]

final = pd.concat(dfs, ignore_index=True).drop_duplicates()
```

```{python}
from functools import partial
from geopy.geocoders import OpenCage
from geopy.extra.rate_limiter import RateLimiter
from tqdm import tqdm
tqdm.pandas()


gc = OpenCage("a3f2b34165cf43d2861c5ebca90fae6b")
geocode = RateLimiter(gc.geocode, min_delay_seconds=0.07)
df['result'] = df['place'].progress_apply(partial(geocode, annotations=False, language='en'))
df["annotations"] = df["result"].apply(lambda x: x.raw if not pd.isna(x) else {})

def flatten_dict(d, parent_key="", sep="_"):
    items = {}
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.update(flatten_dict(v, new_key, sep=sep))
        else:
            items[new_key] = v
    return items

df["flat"] = df["annotations"].apply(lambda x: flatten_dict(x))
df = df.join(df["flat"].apply(pd.Series))
df = df.drop(columns=["flat"])
```


# References

::: {#refs}
:::
